{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a_2d_image_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imageio.v2 as imageio\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_arr = imageio.imread(os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"a_image-dog\", \"bobby.jpg\"))\n",
    "print(type(img_arr))\n",
    "print(img_arr.shape)\n",
    "print(img_arr.dtype)\n",
    "\n",
    "img = torch.from_numpy(img_arr)\n",
    "out = img.permute(2, 0, 1)\n",
    "print(out.shape)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "data_dir = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"b_image-cats\")\n",
    "filenames = [\n",
    "  name for name in os.listdir(data_dir) if os.path.splitext(name)[-1] == '.png'\n",
    "]\n",
    "print(filenames)\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "for i, filename in enumerate(filenames):\n",
    "  image = Image.open(os.path.join(data_dir, filename))\n",
    "  image.show()\n",
    "  img_arr = imageio.imread(os.path.join(data_dir, filename))\n",
    "  print(img_arr.shape)\n",
    "  print(img_arr.dtype)\n",
    "\n",
    "batch_size = 3\n",
    "batch = torch.zeros(batch_size, 3, 256, 256, dtype=torch.uint8)\n",
    "\n",
    "for i, filename in enumerate(filenames):\n",
    "  img_arr = imageio.imread(os.path.join(data_dir, filename))\n",
    "  img_t = torch.from_numpy(img_arr)\n",
    "  img_t = img_t.permute(2, 0, 1)\n",
    "  batch[i] = img_t\n",
    "\n",
    "print(batch.shape)\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "batch = batch.float()\n",
    "batch /= 255.0\n",
    "print(batch.dtype)\n",
    "print(batch.shape)\n",
    "\n",
    "n_channels = batch.shape[1]\n",
    "\n",
    "for c in range(n_channels):\n",
    "  mean = torch.mean(batch[:, c])\n",
    "  std = torch.std(batch[:, c])\n",
    "  print(mean, std)\n",
    "  batch[:, c] = (batch[:, c] - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b_3d_image_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio.v2 as imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"c_volumetric-dicom\", \"2-LUNG_3.0_B70f-04083\")\n",
    "vol_array = imageio.volread(dir_path, format='DICOM')\n",
    "print(type(vol_array))   # >>> <class 'imageio.core.util.Array'>:  Numpy NDArray\n",
    "print(vol_array.shape)   # >>> (99, 512, 512)\n",
    "print(vol_array.dtype)   # >>> int16\n",
    "print(vol_array[0])\n",
    "\n",
    "print(\"#\" * 50, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "for id in range(0, 99):\n",
    "  fig.add_subplot(10, 10, id + 1)\n",
    "  plt.imshow(vol_array[id])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = torch.from_numpy(vol_array).float()\n",
    "vol = torch.unsqueeze(vol, 0)  # channel\n",
    "vol = torch.unsqueeze(vol, 0)  # data size\n",
    "\n",
    "print(vol.shape)  # >>> torch.Size([1, 1, 99, 512, 512])\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "mean = torch.mean(vol, dim=(3, 4), keepdim=True)  # mean over all of dim=(3, 4)\n",
    "print(mean.shape)\n",
    "std = torch.std(vol, dim=(3, 4), keepdim=True)    # std over all of dim=(3, 4)\n",
    "print(std.shape)\n",
    "vol = (vol - mean) / std\n",
    "print(vol.shape)\n",
    "\n",
    "print(vol[0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c_tabular_wind_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_path = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"d_tabular-wine\", \"winequality-white.csv\")\n",
    "wineq_numpy = np.loadtxt(wine_path, dtype=np.float32, delimiter=\";\", skiprows=1)\n",
    "print(wineq_numpy.dtype)\n",
    "print(wineq_numpy.shape)\n",
    "print(wineq_numpy)\n",
    "print()\n",
    "\n",
    "col_list = next(csv.reader(open(wine_path), delimiter=';'))\n",
    "print(col_list)\n",
    "print()\n",
    "\n",
    "print(\"#\" * 50, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wineq = torch.from_numpy(wineq_numpy)\n",
    "print(wineq.dtype)\n",
    "print(wineq.shape)\n",
    "print()\n",
    "\n",
    "data = wineq[:, :-1]  # Selects all rows and all columns except the last\n",
    "print(data.dtype)\n",
    "print(data.shape)\n",
    "print(data)\n",
    "print()\n",
    "\n",
    "target = wineq[:, -1]  # Selects all rows and the last column\n",
    "print(target.dtype)\n",
    "print(target.shape)\n",
    "print(target)\n",
    "print()\n",
    "\n",
    "target = target.long()  # treat labels as an integer\n",
    "print(target.dtype)\n",
    "print(target.shape)\n",
    "print(target)\n",
    "print()\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "eye_matrix = torch.eye(10)\n",
    "# We use the 'target' tensor as indices to extract the corresponding rows from the identity matrix\n",
    "# It can generate the one-hot vectors for each element in the 'target' tensor\n",
    "onehot_target = eye_matrix[target]\n",
    "\n",
    "print(onehot_target.shape)  # >>> torch.Size([4898, 10])\n",
    "print(onehot_target[0])\n",
    "print(onehot_target[1])\n",
    "print(onehot_target[-2])\n",
    "print(onehot_target)\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "data_mean = torch.mean(data, dim=0)\n",
    "data_var = torch.var(data, dim=0)\n",
    "data = (data - data_mean) / torch.sqrt(data_var)\n",
    "print(data)\n",
    "\n",
    "print(\"#\" * 50, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, onehot_target, test_size=0.2)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "def get_wine_data():\n",
    "  wine_path = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"d_tabular-wine\", \"winequality-white.csv\")\n",
    "  wineq_numpy = np.loadtxt(wine_path, dtype=np.float32, delimiter=\";\", skiprows=1)\n",
    "\n",
    "  wineq = torch.from_numpy(wineq_numpy)\n",
    "\n",
    "  data = wineq[:, :-1]  # Selects all rows and all columns except the last\n",
    "  target = wineq[:, -1].long()  # treat labels as an integer\n",
    "\n",
    "  eye_matrix = torch.eye(10)\n",
    "  onehot_target = eye_matrix[target]\n",
    "\n",
    "  data_mean = torch.mean(data, dim=0)\n",
    "  data_var = torch.var(data, dim=0)\n",
    "  data = (data - data_mean) / torch.sqrt(data_var)\n",
    "\n",
    "  X_train, X_valid, y_train, y_valid = train_test_split(data, onehot_target, test_size=0.2)\n",
    "\n",
    "  return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d_tabular_california_housing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/analytics-vidhya/implement-linear-regression-on-boston-housing-dataset-by-pytorch-c5d29546f938\n",
    "# https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset\n",
    "import torch\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "print(housing.keys())\n",
    "\n",
    "print(type(housing.data))\n",
    "print(housing.data.dtype)\n",
    "print(housing.data.shape)\n",
    "print(housing.feature_names)\n",
    "\n",
    "print(housing.target.shape)\n",
    "print(housing.target_names)\n",
    "\n",
    "print(\"#\" * 50, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(housing.data.min(), housing.data.max())\n",
    "\n",
    "data_mean = np.mean(housing.data, axis=0)\n",
    "data_var = np.var(housing.data, axis=0)\n",
    "data = (housing.data - data_mean) / np.sqrt(data_var)\n",
    "target = housing.target\n",
    "\n",
    "print(data.min(), data.max())\n",
    "\n",
    "print(\"#\" * 50, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2)\n",
    "\n",
    "X_train = torch.from_numpy(X_train)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e_bikes_sharing_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_path = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"e_time-series-bike-sharing-dataset\", \"hour-fixed.csv\")\n",
    "\n",
    "bikes_numpy = np.loadtxt(\n",
    "  fname=bikes_path, dtype=np.float32, delimiter=\",\", skiprows=1,\n",
    "  converters={\n",
    "    1: lambda x: float(x[8:10])  # 2011-01-07 --> 07 --> 7.0\n",
    "  }\n",
    ")\n",
    "bikes = torch.from_numpy(bikes_numpy)\n",
    "print(bikes.shape)\n",
    "\n",
    "daily_bikes = bikes.view(-1, 24, bikes.shape[1])\n",
    "print(daily_bikes.shape)  # >>> torch.Size([730, 24, 17])\n",
    "\n",
    "daily_bikes_data = daily_bikes[:, :, :-1]\n",
    "daily_bikes_target = daily_bikes[:, :, -1].unsqueeze(dim=-1)\n",
    "\n",
    "print(daily_bikes_data.shape)\n",
    "print(daily_bikes_target.shape)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "first_day_data = daily_bikes_data[0]\n",
    "print(first_day_data.shape)\n",
    "\n",
    "# Whether situation: 1: clear, 2:mist, 3: light rain/snow, 4: heavy rain/snow\n",
    "print(first_day_data[:, 9].long())\n",
    "eye_matrix = torch.eye(4)\n",
    "print(eye_matrix)\n",
    "\n",
    "weather_onehot = eye_matrix[first_day_data[:, 9].long() - 1]\n",
    "print(weather_onehot.shape)\n",
    "print(weather_onehot)\n",
    "\n",
    "first_day_data_torch = torch.cat(tensors=(first_day_data, weather_onehot), dim=1)\n",
    "print(first_day_data_torch.shape)\n",
    "print(first_day_data_torch)\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "day_data_torch_list = []\n",
    "\n",
    "for daily_idx in range(daily_bikes_data.shape[0]):  # range(730)\n",
    "  day = daily_bikes_data[daily_idx]  # day.shape: [24, 16]\n",
    "  weather_onehot = eye_matrix[day[:, 9].long() - 1]\n",
    "  day_data_torch = torch.cat(tensors=(day, weather_onehot), dim=1)  # day_data_torch.shape: [24, 20]\n",
    "  day_data_torch_list.append(day_data_torch)\n",
    "\n",
    "print(len(day_data_torch_list))\n",
    "daily_bikes_data = torch.stack(day_data_torch_list, dim=0)\n",
    "print(daily_bikes_data.shape)\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "print(daily_bikes_data[:, :, :9].shape, daily_bikes_data[:, :, 10:].shape)\n",
    "daily_bikes_data = torch.cat(\n",
    "  [daily_bikes_data[:, :, 1:9], daily_bikes_data[:, :, 10:]], dim=2\n",
    ") # Drop 'instant' and 'whethersit' columns\n",
    "print(daily_bikes_data.shape)\n",
    "\n",
    "temperatures = daily_bikes_data[:, :, 8]\n",
    "daily_bikes_data[:, :, 8] = (daily_bikes_data[:, :, 8] - torch.mean(temperatures)) / torch.std(temperatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f_hourly_bikes_sharing_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = str(Path(__file__).resolve().parent.parent.parent) # BASE_PATH: /Users/yhhan/git/link_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(edgeitems=2, threshold=50, linewidth=75)\n",
    "\n",
    "bikes_path = os.path.join(BASE_PATH, \"_00_data\", \"e_time-series-bike-sharing-dataset\", \"hour-fixed.csv\")\n",
    "\n",
    "bikes_numpy = np.loadtxt(\n",
    "  fname=bikes_path, dtype=np.float32, delimiter=\",\", skiprows=1,\n",
    "  converters={\n",
    "    1: lambda x: float(x[8:10])  # 2011-01-07 --> 07 --> 7\n",
    "  }\n",
    ")\n",
    "bikes_data = torch.from_numpy(bikes_numpy).to(torch.float)\n",
    "print(bikes_data.shape)    # >>> torch.Size([17520, 17])\n",
    "bikes_target = bikes_data[:, -1].unsqueeze(dim=-1)  # 'cnt'\n",
    "bikes_data = bikes_data[:, :-1]   # >>> torch.Size([17520, 16])\n",
    "\n",
    "eye_matrix = torch.eye(4)\n",
    "\n",
    "data_torch_list = []\n",
    "for idx in range(bikes_data.shape[0]):  # range(730)\n",
    "  hour_data = bikes_data[idx]  # hour_data.shape: [17]\n",
    "  weather_onehot = eye_matrix[hour_data[9].long() - 1]\n",
    "  concat_data_torch = torch.cat(tensors=(hour_data, weather_onehot), dim=-1)\n",
    "  # concat_data_torch.shape: [20]\n",
    "  data_torch_list.append(concat_data_torch)\n",
    "\n",
    "bikes_data = torch.stack(data_torch_list, dim=0)\n",
    "bikes_data = torch.cat([bikes_data[:, 1:9], bikes_data[:, 10:]], dim=-1)\n",
    "# Drop 'instant' and 'whethersit' columns\n",
    "\n",
    "print(bikes_data.shape)\n",
    "print(bikes_data[0])\n",
    "\n",
    "#################################################################################################\n",
    "\n",
    "sequence_size = 24\n",
    "validation_size = 96\n",
    "test_size = 24\n",
    "y_normalizer = 100\n",
    "\n",
    "data_size = len(bikes_data) - sequence_size + 1\n",
    "print(\"data_size: {0}\".format(data_size))\n",
    "train_size = data_size - (validation_size + test_size)\n",
    "print(\"train_size: {0}, validation_size: {1}, test_size: {2}\".format(train_size, validation_size, test_size))\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "#################################################################################################\n",
    "\n",
    "row_cursor = 0\n",
    "\n",
    "X_train_list = []\n",
    "y_train_regression_list = []\n",
    "for idx in range(0, train_size):\n",
    "  sequence_data = bikes_data[idx: idx + sequence_size]\n",
    "  sequence_target = bikes_target[idx + sequence_size - 1]\n",
    "  X_train_list.append(sequence_data)\n",
    "  y_train_regression_list.append(sequence_target)\n",
    "  row_cursor += 1\n",
    "\n",
    "X_train = torch.stack(X_train_list, dim=0).to(torch.float)\n",
    "print(X_train.shape)\n",
    "y_train_regression = torch.tensor(y_train_regression_list, dtype=torch.float32) / y_normalizer\n",
    "\n",
    "m = X_train.mean(dim=0, keepdim=True)\n",
    "s = X_train.std(dim=0, keepdim=True)\n",
    "X_train = (X_train - m) / s\n",
    "\n",
    "print(X_train.shape, y_train_regression.shape)\n",
    "# >>> torch.Size([17376, 24, 19]) torch.Size([17376])\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "#################################################################################################\n",
    "\n",
    "X_validation_list = []\n",
    "y_validation_regression_list = []\n",
    "for idx in range(row_cursor, row_cursor + validation_size):\n",
    "  sequence_data = bikes_data[idx: idx + sequence_size]\n",
    "  sequence_target = bikes_target[idx + sequence_size - 1]\n",
    "  X_validation_list.append(sequence_data)\n",
    "  y_validation_regression_list.append(sequence_target)\n",
    "  row_cursor += 1\n",
    "\n",
    "X_validation = torch.stack(X_validation_list, dim=0).to(torch.float)\n",
    "y_validation_regression = torch.tensor(y_validation_regression_list, dtype=torch.float32) / y_normalizer\n",
    "\n",
    "X_validation = (X_validation - m) / s\n",
    "\n",
    "print(X_validation.shape, y_validation_regression.shape)\n",
    "# >>> torch.Size([96, 24, 19]) torch.Size([96])\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "#################################################################################################\n",
    "\n",
    "X_test_list = []\n",
    "y_test_regression_list = []\n",
    "for idx in range(row_cursor, row_cursor + test_size):\n",
    "  sequence_data = bikes_data[idx: idx + sequence_size]\n",
    "  sequence_target = bikes_target[idx + sequence_size - 1]\n",
    "  X_test_list.append(sequence_data)\n",
    "  y_test_regression_list.append(sequence_target)\n",
    "  row_cursor += 1\n",
    "\n",
    "X_test = torch.stack(X_test_list, dim=0).to(torch.float)\n",
    "y_test_regression = torch.tensor(y_test_regression_list, dtype=torch.float32) / y_normalizer\n",
    "\n",
    "X_test -= (X_test - m) / s\n",
    "\n",
    "print(X_test.shape, y_test_regression.shape)\n",
    "# >>> torch.Size([24, 24, 18]) torch.Size([24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# g_cryptocurrency_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://finance.yahoo.com/quote/BTC-KRW/history/\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = str(Path(__file__).resolve().parent.parent.parent) # BASE_PATH: /Users/yhhan/git/link_dl\n",
    "import sys\n",
    "sys.path.append(BASE_PATH)\n",
    "\n",
    "\n",
    "btc_krw_path = os.path.join(BASE_PATH, \"_00_data\", \"k_cryptocurrency\", \"BTC_KRW.csv\")\n",
    "df = pd.read_csv(btc_krw_path)\n",
    "print(df)\n",
    "\n",
    "row_size = len(df)\n",
    "print(\"row_size:\", row_size)\n",
    "\n",
    "columns = df.columns  #['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
    "print([column for column in columns])\n",
    "date_list = df['Date']\n",
    "df = df.drop(columns=['Date'])\n",
    "\n",
    "print(df)\n",
    "print(\"#\" * 100, 0)\n",
    "\n",
    "#################################################################################################\n",
    "\n",
    "sequence_size = 10\n",
    "validation_size = 100\n",
    "test_size = 50\n",
    "\n",
    "data_size = row_size - sequence_size + 1\n",
    "print(\"data_size: {0}\".format(data_size))\n",
    "train_size = data_size - (validation_size + test_size)\n",
    "print(\"train_size: {0}, validation_size: {1}, test_size: {2}\".format(train_size, validation_size, test_size))\n",
    "\n",
    "print(\"#\" * 100, 1)\n",
    "\n",
    "#################################################################################################\n",
    "\n",
    "row_cursor = 0\n",
    "y_normalizer = 1.0e7\n",
    "\n",
    "X_train_list = []\n",
    "y_train_regression_list = []\n",
    "y_train_classification_list = []\n",
    "y_train_date = []\n",
    "for idx in range(0, train_size):\n",
    "  sequence_data = df.iloc[idx: idx + sequence_size].values  # sequence_data.shape: (sequence_size, 5)\n",
    "  X_train_list.append(torch.from_numpy(sequence_data))\n",
    "  y_train_regression_list.append(df.iloc[idx + sequence_size - 1][\"Close\"])\n",
    "  y_train_classification_list.append(\n",
    "    1 if df.iloc[idx + sequence_size - 1][\"Close\"] >= df.iloc[idx + sequence_size - 2][\"Close\"] else 0\n",
    "  )\n",
    "  y_train_date.append(date_list[idx + sequence_size - 1])\n",
    "  row_cursor += 1\n",
    "\n",
    "X_train = torch.stack(X_train_list, dim=0).to(torch.float)\n",
    "y_train_regression = torch.tensor(y_train_regression_list, dtype=torch.float32) / y_normalizer\n",
    "y_train_classification = torch.tensor(y_train_classification_list, dtype=torch.int64)\n",
    "print(y_train_classification)\n",
    "\n",
    "m = X_train.mean(dim=0, keepdim=True)\n",
    "s = X_train.std(dim=0, keepdim=True)\n",
    "X_train -= m\n",
    "X_train /= s\n",
    "print(X_train.shape, y_train_regression.shape, y_train_classification.shape)\n",
    "print(\"Label - Start Date: {0} ~ End Date: {1}\".format(y_train_date[0], y_train_date[-1]))\n",
    "\n",
    "print(\"#\" * 100, 2)\n",
    "\n",
    "#################################################################################################\n",
    "\n",
    "X_validation_list = []\n",
    "y_validation_regression_list = []\n",
    "y_validation_classification_list = []\n",
    "y_validation_date = []\n",
    "for idx in range(row_cursor, row_cursor + validation_size):\n",
    "  sequence_data = df.iloc[idx: idx + sequence_size].values     # sequence_data.shape: (sequence_size, 5)\n",
    "  X_validation_list.append(torch.from_numpy(sequence_data))\n",
    "  y_validation_regression_list.append(df.iloc[idx + sequence_size - 1][\"Close\"])\n",
    "  y_validation_classification_list.append(\n",
    "    1 if df.iloc[idx + sequence_size - 1][\"Close\"] >= df.iloc[idx + sequence_size - 2][\"Close\"] else 0\n",
    "  )\n",
    "  y_validation_date.append(date_list[idx + sequence_size - 1])\n",
    "  row_cursor += 1\n",
    "\n",
    "X_validation = torch.stack(X_validation_list, dim=0).to(torch.float)\n",
    "y_validation_regression = torch.tensor(y_validation_regression_list, dtype=torch.float32) / y_normalizer\n",
    "y_validation_classification = torch.tensor(y_validation_classification_list, dtype=torch.int64)\n",
    "print(y_validation_classification)\n",
    "\n",
    "X_validation = (X_validation - m) / s\n",
    "print(X_validation.shape, y_validation_regression.shape, y_validation_classification.shape)\n",
    "print(\"Label - Start Date: {0} ~ End Date: {1}\".format(y_validation_date[0], y_validation_date[-1]))\n",
    "\n",
    "print(\"#\" * 100, 3)\n",
    "\n",
    "#################################################################################################\n",
    "\n",
    "X_test_list = []\n",
    "y_test_regression_list = []\n",
    "y_test_classification_list = []\n",
    "y_test_date = []\n",
    "for idx in range(row_cursor, row_cursor + test_size):\n",
    "  sequence_data = df.iloc[idx: idx + sequence_size].values   # sequence_data.shape: (sequence_size, 5)\n",
    "  X_test_list.append(torch.from_numpy(sequence_data))\n",
    "  y_test_regression_list.append(df.iloc[idx + sequence_size - 1][\"Close\"])\n",
    "  y_test_classification_list.append(\n",
    "    1 if df.iloc[idx + sequence_size - 1][\"Close\"] > df.iloc[idx + sequence_size - 2][\"Close\"] else 0\n",
    "  )\n",
    "  y_test_date.append(date_list[idx + sequence_size - 1])\n",
    "  row_cursor += 1\n",
    "\n",
    "X_test = torch.stack(X_test_list, dim=0).to(torch.float)\n",
    "y_test_regression = torch.tensor(y_test_regression_list, dtype=torch.float32) / y_normalizer\n",
    "y_test_classification = torch.tensor(y_test_classification_list, dtype=torch.int64)\n",
    "print(y_test_classification)\n",
    "X_test = (X_test - m) / s\n",
    "print(X_test.shape, y_test_regression.shape, y_test_classification.shape)\n",
    "print(\"Label - Start Date: {0} ~ End Date: {1}\".format(y_test_date[0], y_test_date[-1]))\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(13, 7))\n",
    "ax.plot(y_train_date, y_train_regression * y_normalizer, label=\"y_train_regression\", linewidth=2)\n",
    "ax.plot(y_validation_date, y_validation_regression * y_normalizer, label=\"y_validation\", linewidth=2)\n",
    "ax.plot(y_test_date, y_test_regression * y_normalizer, label=\"y_test\", linewidth=2)\n",
    "ax.set_ylabel('Bitcoin [KRW]', fontsize=14)\n",
    "ax.set_xticks(ax.get_xticks()[::200])\n",
    "plt.ticklabel_format(style='plain', axis='y')\n",
    "plt.xticks(rotation=25)\n",
    "ax.legend(loc='upper left', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# h_audio_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import scipy.io.wavfile as wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_1_path = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"f_audio-chirp\", \"1-100038-A-14.wav\")\n",
    "audio_2_path = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"f_audio-chirp\", \"1-100210-A-36.wav\")\n",
    "\n",
    "freq_1, waveform_arr_1 = wavfile.read(audio_1_path)\n",
    "print(freq_1)\n",
    "print(type(waveform_arr_1))\n",
    "print(len(waveform_arr_1))\n",
    "print(waveform_arr_1)\n",
    "\n",
    "freq_2, waveform_arr_2 = wavfile.read(audio_2_path)\n",
    "\n",
    "waveform = torch.empty(2, 1, 220_500)\n",
    "waveform[0, 0] = torch.from_numpy(waveform_arr_1).float()\n",
    "waveform[1, 0] = torch.from_numpy(waveform_arr_2).float()\n",
    "print(waveform.shape)\n",
    "\n",
    "print(\"#\" * 50, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, sp_arr_1 = signal.spectrogram(waveform_arr_1, freq_1)\n",
    "_, _, sp_arr_2 = signal.spectrogram(waveform_arr_2, freq_2)\n",
    "\n",
    "sp_1 = torch.from_numpy(sp_arr_1)\n",
    "sp_2 = torch.from_numpy(sp_arr_2)\n",
    "print(sp_1.shape)\n",
    "print(sp_2.shape)\n",
    "\n",
    "sp_left_t = torch.from_numpy(sp_arr_1)\n",
    "sp_right_t = torch.from_numpy(sp_arr_2)\n",
    "print(sp_left_t.shape)\n",
    "print(sp_right_t.shape)\n",
    "\n",
    "sp_t = torch.stack((sp_left_t, sp_right_t), dim=0).unsqueeze(dim=0)\n",
    "print(sp_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# i_video_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install imageio[ffmpeg]\n",
    "import torch\n",
    "import os\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"g_video-cockatoo\", \"cockatoo.mp4\")\n",
    "\n",
    "reader = imageio.get_reader(video_path)\n",
    "print(type(reader))\n",
    "meta = reader.get_meta_data()\n",
    "print(meta)\n",
    "\n",
    "for i, frame in enumerate(reader):\n",
    "  frame = torch.from_numpy(frame).float()  # frame.shape: [360, 480, 3]\n",
    "  print(i, frame.shape)   # i, torch.Size([360, 480, 3])\n",
    "\n",
    "n_channels = 3\n",
    "n_frames = 529\n",
    "video = torch.empty(1, n_frames, n_channels, *meta['size'])  # (1, 529, 3, 480, 360)\n",
    "print(video.shape)\n",
    "\n",
    "for i, frame in enumerate(reader):\n",
    "  frame = torch.from_numpy(frame).float()       # frame.shape: [360, 480, 3]\n",
    "  frame = torch.permute(frame, dims=(2, 1, 0))  # frame.shape: [3, 480, 360]\n",
    "  video[0, i] = frame\n",
    "\n",
    "video = video.permute(dims=(0, 2, 1, 3, 4))\n",
    "print(video.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# j_linear_regression_dataset_dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionDataset(Dataset):\n",
    "  def __init__(self, N=50, m=-3, b=2, *args, **kwargs):\n",
    "    # N: number of samples, e.g. 50\n",
    "    # m: slope\n",
    "    # b: offset\n",
    "    super().__init__(*args, **kwargs)\n",
    "\n",
    "    self.x = torch.rand(N, 2)\n",
    "    self.noise = torch.rand(N) * 0.2\n",
    "    self.m = m\n",
    "    self.b = b\n",
    "    self.y = (torch.sum(self.x * self.m) + self.b + self.noise).unsqueeze(-1)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.x)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.x[idx], self.y[idx]\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "      len(self.x), self.x.shape, self.y.shape\n",
    "    )\n",
    "    return str\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  linear_regression_dataset = LinearRegressionDataset()\n",
    "\n",
    "  print(linear_regression_dataset)\n",
    "\n",
    "  print(\"#\" * 50, 1)\n",
    "\n",
    "  for idx, sample in enumerate(linear_regression_dataset):\n",
    "    input, target = sample\n",
    "    print(\"{0} - {1}: {2}\".format(idx, input, target))\n",
    "\n",
    "  train_dataset, validation_dataset, test_dataset = random_split(linear_regression_dataset, [0.7, 0.2, 0.1])\n",
    "\n",
    "  print(\"#\" * 50, 2)\n",
    "\n",
    "  print(len(train_dataset), len(validation_dataset), len(test_dataset))\n",
    "\n",
    "  print(\"#\" * 50, 3)\n",
    "\n",
    "  train_data_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True\n",
    "  )\n",
    "\n",
    "  for idx, batch in enumerate(train_data_loader):\n",
    "    input, target = batch\n",
    "    print(\"{0} - {1}: {2}\".format(idx, input, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k_2d_image_dataset_dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogCat2DImageDataset(Dataset):\n",
    "  def __init__(self):\n",
    "    self.image_transforms = transforms.Compose([\n",
    "      transforms.Resize(size=(256, 256)),\n",
    "      transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    dogs_dir = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"a_image-dog\")\n",
    "    cats_dir = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"b_image-cats\")\n",
    "\n",
    "    image_lst = [\n",
    "      Image.open(os.path.join(dogs_dir, \"bobby.jpg\")),  # (1280, 720, 3)\n",
    "      Image.open(os.path.join(cats_dir, \"cat1.png\")),  # (256, 256, 3)\n",
    "      Image.open(os.path.join(cats_dir, \"cat2.png\")),  # (256, 256, 3)\n",
    "      Image.open(os.path.join(cats_dir, \"cat3.png\"))  # (256, 256, 3)\n",
    "    ]\n",
    "\n",
    "    image_lst = [self.image_transforms(img) for img in image_lst]\n",
    "    self.images = torch.stack(image_lst, dim=0)\n",
    "\n",
    "    # 0: \"dog\", 1: \"cat\"\n",
    "    self.image_labels = torch.tensor([[0], [1], [1], [1]])\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.images)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.images[idx], self.image_labels[idx]\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "      len(self.images), self.images.shape, self.image_labels.shape\n",
    "    )\n",
    "    return str\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  dog_cat_2d_image_dataset = DogCat2DImageDataset()\n",
    "\n",
    "  print(dog_cat_2d_image_dataset)\n",
    "\n",
    "  print(\"#\" * 50, 1)\n",
    "\n",
    "  for idx, sample in enumerate(dog_cat_2d_image_dataset):\n",
    "    input, target = sample\n",
    "    print(\"{0} - {1}: {2}\".format(idx, input.shape, target))\n",
    "\n",
    "  train_dataset, test_dataset = random_split(dog_cat_2d_image_dataset, [0.7, 0.3])\n",
    "\n",
    "  print(\"#\" * 50, 2)\n",
    "\n",
    "  print(len(train_dataset), len(test_dataset))\n",
    "\n",
    "  print(\"#\" * 50, 3)\n",
    "\n",
    "  train_data_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True\n",
    "  )\n",
    "\n",
    "  for idx, batch in enumerate(train_data_loader):\n",
    "    input, target = batch\n",
    "    print(\"{0} - {1}: {2}\".format(idx, input.shape, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# l_wine_dataset_dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WineDataset(Dataset):\n",
    "  def __init__(self):\n",
    "    wine_path = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"d_tabular-wine\", \"winequality-white.csv\")\n",
    "    wineq_numpy = np.loadtxt(wine_path, dtype=np.float32, delimiter=\";\", skiprows=1)\n",
    "    wineq = torch.from_numpy(wineq_numpy)\n",
    "\n",
    "    data = wineq[:, :-1]  # Selects all rows and all columns except the last\n",
    "    data_mean = torch.mean(data, dim=0)\n",
    "    data_var = torch.var(data, dim=0)\n",
    "    self.data = (data - data_mean) / torch.sqrt(data_var)\n",
    "\n",
    "    target = wineq[:, -1].long()  # treat labels as an integer\n",
    "    eye_matrix = torch.eye(10)\n",
    "    self.target = eye_matrix[target]\n",
    "\n",
    "    assert len(self.data) == len(self.target)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    wine_feature = self.data[idx]\n",
    "    wine_target = self.target[idx]\n",
    "    return wine_feature, wine_target\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "      len(self.data), self.data.shape, self.target.shape\n",
    "    )\n",
    "    return str\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  wine_dataset = WineDataset()\n",
    "\n",
    "  print(wine_dataset)\n",
    "\n",
    "  print(\"#\" * 50, 1)\n",
    "\n",
    "  for idx, sample in enumerate(wine_dataset):\n",
    "    input, target = sample\n",
    "    print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))\n",
    "\n",
    "  train_dataset, validation_dataset, test_dataset = random_split(wine_dataset, [0.7, 0.2, 0.1])\n",
    "\n",
    "  print(\"#\" * 50, 2)\n",
    "\n",
    "  print(len(train_dataset), len(validation_dataset), len(test_dataset))\n",
    "\n",
    "  print(\"#\" * 50, 3)\n",
    "\n",
    "  train_data_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    "  )\n",
    "\n",
    "  for idx, batch in enumerate(train_data_loader):\n",
    "    input, target = batch\n",
    "    print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# m_california_housing_dataset_dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaliforniaHousingDataset(Dataset):\n",
    "  def __init__(self):\n",
    "    from sklearn.datasets import fetch_california_housing\n",
    "    housing = fetch_california_housing()\n",
    "    data_mean = np.mean(housing.data, axis=0)\n",
    "    data_var = np.var(housing.data, axis=0)\n",
    "    self.data = torch.tensor((housing.data - data_mean) / np.sqrt(data_var), dtype=torch.float32)\n",
    "    self.target = torch.tensor(housing.target, dtype=torch.float32).unsqueeze(dim=-1)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    sample_data = self.data[idx]\n",
    "    sample_target = self.target[idx]\n",
    "    return sample_data, sample_target\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "      len(self.data), self.data.shape, self.target.shape\n",
    "    )\n",
    "    return str\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  california_housing_dataset = CaliforniaHousingDataset()\n",
    "\n",
    "  print(california_housing_dataset)\n",
    "\n",
    "  print(\"#\" * 50, 1)\n",
    "\n",
    "  for idx, sample in enumerate(california_housing_dataset):\n",
    "    input, target = sample\n",
    "    print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))\n",
    "\n",
    "  train_dataset, validation_dataset, test_dataset = random_split(california_housing_dataset, [0.7, 0.2, 0.1])\n",
    "\n",
    "  print(\"#\" * 50, 2)\n",
    "\n",
    "  print(len(train_dataset), len(validation_dataset), len(test_dataset))\n",
    "\n",
    "  print(\"#\" * 50, 3)\n",
    "\n",
    "  train_data_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    "  )\n",
    "\n",
    "  for idx, batch in enumerate(train_data_loader):\n",
    "    input, target = batch\n",
    "    print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n_time_series_dataset_dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = str(Path(__file__).resolve().parent.parent.parent) # BASE_PATH: /Users/yhhan/git/link_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BikesDataset(Dataset):\n",
    "  def __init__(self, train=True, test_days=1):\n",
    "    self.train = train\n",
    "    self.test_days = test_days\n",
    "\n",
    "    bikes_path = os.path.join(BASE_PATH, \"_00_data\", \"e_time-series-bike-sharing-dataset\", \"hour-fixed.csv\")\n",
    "\n",
    "    bikes_numpy = np.loadtxt(\n",
    "      fname=bikes_path, dtype=np.float32, delimiter=\",\", skiprows=1,\n",
    "      converters={\n",
    "        1: lambda x: float(x[8:10])  # 2011-01-07 --> 07 --> 7\n",
    "      }\n",
    "    )\n",
    "    bikes = torch.from_numpy(bikes_numpy)\n",
    "\n",
    "    daily_bikes = bikes.view(-1, 24, bikes.shape[1])  # daily_bikes.shape: torch.Size([730, 24, 17])\n",
    "    self.daily_bikes_target = daily_bikes[:, :, -1].unsqueeze(dim=-1)\n",
    "\n",
    "    self.daily_bikes_data = daily_bikes[:, :, :-1]\n",
    "    eye_matrix = torch.eye(4)\n",
    "\n",
    "    day_data_torch_list = []\n",
    "    for daily_idx in range(self.daily_bikes_data.shape[0]):  # range(730)\n",
    "      day = self.daily_bikes_data[daily_idx]  # day.shape: [24, 17]\n",
    "      weather_onehot = eye_matrix[day[:, 9].long() - 1]\n",
    "      day_data_torch = torch.cat(tensors=(day, weather_onehot), dim=1)  # day_torch.shape: [24, 21]\n",
    "      day_data_torch_list.append(day_data_torch)\n",
    "\n",
    "    self.daily_bikes_data = torch.stack(day_data_torch_list, dim=0)\n",
    "\n",
    "    self.daily_bikes_data = torch.cat(\n",
    "      [self.daily_bikes_data[:, :, :9], self.daily_bikes_data[:, :, 10:]], dim=2\n",
    "    )\n",
    "\n",
    "    total_length = len(self.daily_bikes_data)\n",
    "    self.train_bikes_data = self.daily_bikes_data[:total_length - test_days]\n",
    "    self.train_bikes_targets = self.daily_bikes_target[:total_length - test_days]\n",
    "    train_temperatures = self.train_bikes_data[:, :, 9]\n",
    "    train_temperatures_mean = torch.mean(train_temperatures)\n",
    "    train_temperatures_std = torch.std(train_temperatures)\n",
    "    self.train_bikes_data[:, :, 9] = \\\n",
    "      (self.train_bikes_data[:, :, 9] - torch.mean(train_temperatures_mean)) / torch.std(train_temperatures_std)\n",
    "\n",
    "    assert len(self.train_bikes_data) == len(self.train_bikes_targets)\n",
    "\n",
    "    self.test_bikes_data = self.daily_bikes_data[-test_days:]\n",
    "    self.test_bikes_targets = self.daily_bikes_target[-test_days:]\n",
    "\n",
    "    self.test_bikes_data[:, :, 9] = \\\n",
    "      (self.test_bikes_data[:, :, 9] - torch.mean(train_temperatures_mean)) / torch.std(train_temperatures_std)\n",
    "\n",
    "    assert len(self.test_bikes_data) == len(self.test_bikes_targets)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.train_bikes_data) if self.train is True else len(self.test_bikes_data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    bike_feature = self.train_bikes_data[idx] if self.train is True else self.test_bikes_data[idx]\n",
    "    bike_target = self.train_bikes_targets[idx] if self.train is True else self.test_bikes_targets[idx]\n",
    "    return bike_feature, bike_target\n",
    "\n",
    "  def __str__(self):\n",
    "    if self.train is True:\n",
    "      str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "        len(self.train_bikes_data), self.train_bikes_data.shape, self.train_bikes_targets.shape\n",
    "      )\n",
    "    else:\n",
    "      str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "        len(self.test_bikes_data), self.test_bikes_data.shape, self.test_bikes_targets.shape\n",
    "      )\n",
    "    return str\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  train_bikes_dataset = BikesDataset(train=True, test_days=1)\n",
    "  print(train_bikes_dataset)\n",
    "\n",
    "  print(\"#\" * 50, 1)\n",
    "\n",
    "  train_dataset, validation_dataset = random_split(train_bikes_dataset, [0.8, 0.2])\n",
    "\n",
    "  print(\"[TRAIN]\")\n",
    "  for idx, sample in enumerate(train_dataset):\n",
    "    input, target = sample\n",
    "    print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))\n",
    "\n",
    "  train_data_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "\n",
    "  for idx, batch in enumerate(train_data_loader):\n",
    "    input, target = batch\n",
    "    print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))\n",
    "\n",
    "  print(\"#\" * 50, 2)\n",
    "\n",
    "  print(\"[VALIDATION]\")\n",
    "  for idx, sample in enumerate(validation_dataset):\n",
    "    input, target = sample\n",
    "    print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))\n",
    "\n",
    "  validation_data_loader = DataLoader(dataset=validation_dataset, batch_size=32)\n",
    "\n",
    "  for idx, batch in enumerate(validation_data_loader):\n",
    "    input, target = batch\n",
    "    print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))\n",
    "\n",
    "  print(\"#\" * 50, 3)\n",
    "\n",
    "  test_dataset = BikesDataset(train=False, test_days=1)\n",
    "  print(test_dataset)\n",
    "\n",
    "  print(\"[TEST]\")\n",
    "  for idx, sample in enumerate(test_dataset):\n",
    "    input, target = sample\n",
    "    print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))\n",
    "\n",
    "  test_data_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "  for idx, batch in enumerate(test_data_loader):\n",
    "    input, target = batch\n",
    "    print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# o_hourly_bikes_sharing_dataset_dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = str(Path(__file__).resolve().parent.parent.parent) # BASE_PATH: /Users/yhhan/git/link_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HourlyBikesDataset(Dataset):\n",
    "  def __init__(self, X, y):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "\n",
    "    assert len(self.X) == len(self.y)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    X = self.X[idx]\n",
    "    y = self.y[idx]\n",
    "    return X, y\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "      len(self.X), self.X.shape, self.y.shape\n",
    "    )\n",
    "    return str\n",
    "\n",
    "\n",
    "def get_hourly_bikes_data(sequence_size=24, validation_size=96, test_size=24, y_normalizer=100):\n",
    "  bikes_path = os.path.join(BASE_PATH, \"_00_data\", \"e_time-series-bike-sharing-dataset\", \"hour-fixed.csv\")\n",
    "\n",
    "  bikes_numpy = np.loadtxt(\n",
    "    fname=bikes_path, dtype=np.float32, delimiter=\",\", skiprows=1,\n",
    "    converters={\n",
    "      1: lambda x: float(x[8:10])  # 2011-01-07 --> 07 --> 7\n",
    "    }\n",
    "  )\n",
    "  bikes_data = torch.from_numpy(bikes_numpy).to(torch.float) # >>> torch.Size([17520, 17])\n",
    "  bikes_target = bikes_data[:, -1].unsqueeze(dim=-1)  # 'cnt'\n",
    "  bikes_data = bikes_data[:, :-1]  # >>> torch.Size([17520, 16])\n",
    "\n",
    "  eye_matrix = torch.eye(4)\n",
    "\n",
    "  data_torch_list = []\n",
    "  for idx in range(bikes_data.shape[0]):  # range(730)\n",
    "    hour_data = bikes_data[idx]  # day.shape: [24, 17]\n",
    "    weather_onehot = eye_matrix[hour_data[9].long() - 1]\n",
    "    concat_data_torch = torch.cat(tensors=(hour_data, weather_onehot), dim=-1)  # day_torch.shape: [24, 21]\n",
    "    data_torch_list.append(concat_data_torch)\n",
    "\n",
    "  bikes_data = torch.stack(data_torch_list, dim=0)\n",
    "  bikes_data = torch.cat([bikes_data[:, 1:9], bikes_data[:, 10:]], dim=-1)\n",
    "  print(bikes_data.shape, \"!!!\")  # >>> torch.Size([17520, 18])\n",
    "\n",
    "  data_size = len(bikes_data) - sequence_size\n",
    "  train_size = data_size - (validation_size + test_size)\n",
    "\n",
    "  #################################################################################################\n",
    "\n",
    "  row_cursor = 0\n",
    "\n",
    "  X_train_list = []\n",
    "  y_train_regression_list = []\n",
    "  for idx in range(0, train_size):\n",
    "    sequence_data = bikes_data[idx: idx + sequence_size]\n",
    "    sequence_target = bikes_target[idx + sequence_size - 1]\n",
    "    X_train_list.append(sequence_data)\n",
    "    y_train_regression_list.append(sequence_target)\n",
    "    row_cursor += 1\n",
    "\n",
    "  X_train = torch.stack(X_train_list, dim=0).to(torch.float)\n",
    "  y_train_regression = torch.tensor(y_train_regression_list, dtype=torch.float32) / y_normalizer\n",
    "\n",
    "  m = X_train.mean(dim=0, keepdim=True)\n",
    "  s = X_train.std(dim=0, keepdim=True)\n",
    "  X_train = (X_train - m) / s\n",
    "\n",
    "  #################################################################################################\n",
    "\n",
    "  X_validation_list = []\n",
    "  y_validation_regression_list = []\n",
    "  for idx in range(row_cursor, row_cursor + validation_size):\n",
    "    sequence_data = bikes_data[idx: idx + sequence_size]\n",
    "    sequence_target = bikes_target[idx + sequence_size - 1]\n",
    "    X_validation_list.append(sequence_data)\n",
    "    y_validation_regression_list.append(sequence_target)\n",
    "    row_cursor += 1\n",
    "\n",
    "  X_validation = torch.stack(X_validation_list, dim=0).to(torch.float)\n",
    "  y_validation_regression = torch.tensor(y_validation_regression_list, dtype=torch.float32) / y_normalizer\n",
    "\n",
    "  X_validation -= m\n",
    "  X_validation /= s\n",
    "  #################################################################################################\n",
    "\n",
    "  X_test_list = []\n",
    "  y_test_regression_list = []\n",
    "  for idx in range(row_cursor, row_cursor + test_size):\n",
    "    sequence_data = bikes_data[idx: idx + sequence_size]\n",
    "    sequence_target = bikes_target[idx + sequence_size - 1]\n",
    "    X_test_list.append(sequence_data)\n",
    "    y_test_regression_list.append(sequence_target)\n",
    "    row_cursor += 1\n",
    "\n",
    "  X_test = torch.stack(X_test_list, dim=0).to(torch.float)\n",
    "  y_test_regression = torch.tensor(y_test_regression_list, dtype=torch.float32) / y_normalizer\n",
    "\n",
    "  X_test -= m\n",
    "  X_test /= s\n",
    "\n",
    "  return (\n",
    "    X_train, X_validation, X_test,\n",
    "    y_train_regression, y_validation_regression, y_test_regression\n",
    "  )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  X_train, X_validation, X_test, y_train, y_validation, y_test = get_hourly_bikes_data(\n",
    "    sequence_size=24, validation_size=96, test_size=24, y_normalizer=100\n",
    "  )\n",
    "\n",
    "  print(\"Train: {0}, Validation: {1}, Test: {2}\".format(len(X_train), len(X_validation), len(X_test)))\n",
    "\n",
    "  train_hourly_bikes_dataset = HourlyBikesDataset(X=X_train, y=y_train)\n",
    "  validation_hourly_bikes_dataset = HourlyBikesDataset(X=X_validation, y=y_validation)\n",
    "  test_houly_bikes_dataset = HourlyBikesDataset(X=X_test, y=y_test)\n",
    "\n",
    "  train_data_loader = DataLoader(\n",
    "    dataset=train_hourly_bikes_dataset, batch_size=32, shuffle=True, drop_last=True\n",
    "  )\n",
    "\n",
    "  # for idx, batch in enumerate(train_data_loader):\n",
    "  #   input, target = batch\n",
    "  #   print(\"{0} - {1}: {2}, {3}\".format(idx, input.shape, target.shape, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p_cryptocurrency_dataset_dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/cryptocurrency-price-prediction-using-deep-learning-70cfca50dd3a\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = str(Path(__file__).resolve().parent.parent.parent) # BASE_PATH: /Users/yhhan/git/link_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cryptocurrency_data(\n",
    "    sequence_size=10, validation_size=100, test_size=10, target_column='Close', y_normalizer=1.0e7, is_regression=True\n",
    "):\n",
    "  btc_krw_path = os.path.join(BASE_PATH, \"_00_data\", \"k_cryptocurrency\", \"BTC_KRW.csv\")\n",
    "  df = pd.read_csv(btc_krw_path)\n",
    "  row_size = len(df)\n",
    "  # ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
    "  date_list = df['Date']\n",
    "\n",
    "  df = df.drop(columns=['Date'])\n",
    "\n",
    "  data_size = row_size - sequence_size\n",
    "  train_size = data_size - (validation_size + test_size)\n",
    "  #################################################################################################\n",
    "\n",
    "  row_cursor = 0\n",
    "\n",
    "  X_train_list = []\n",
    "  y_train_regression_list = []\n",
    "  y_train_classification_list = []\n",
    "  y_train_date = []\n",
    "  for idx in range(0, train_size):\n",
    "    sequence_data = df.iloc[idx: idx + sequence_size].values  # sequence_data.shape: (sequence_size, 5)\n",
    "    X_train_list.append(torch.from_numpy(sequence_data))\n",
    "    y_train_regression_list.append(df.iloc[idx + sequence_size][target_column])\n",
    "    y_train_classification_list.append(\n",
    "      1 if df.iloc[idx + sequence_size][target_column] >= df.iloc[idx + sequence_size - 1][target_column] else 0\n",
    "    )\n",
    "    y_train_date.append(date_list[idx + sequence_size])\n",
    "    row_cursor += 1\n",
    "\n",
    "  X_train = torch.stack(X_train_list, dim=0).to(torch.float)\n",
    "  y_train_regression = torch.tensor(y_train_regression_list, dtype=torch.float32) / y_normalizer\n",
    "  y_train_classification = torch.tensor(y_train_classification_list, dtype=torch.int64)\n",
    "\n",
    "  m = X_train.mean(dim=0, keepdim=True)\n",
    "  s = X_train.std(dim=0, keepdim=True)\n",
    "  X_train = (X_train - m) / s\n",
    "\n",
    "  #################################################################################################\n",
    "\n",
    "  X_validation_list = []\n",
    "  y_validation_regression_list = []\n",
    "  y_validation_classification_list = []\n",
    "  y_validation_date = []\n",
    "  for idx in range(row_cursor, row_cursor + validation_size):\n",
    "    sequence_data = df.iloc[idx: idx + sequence_size].values  # sequence_data.shape: (sequence_size, 5)\n",
    "    X_validation_list.append(torch.from_numpy(sequence_data))\n",
    "    y_validation_regression_list.append(df.iloc[idx + sequence_size][target_column])\n",
    "    y_validation_classification_list.append(\n",
    "      1 if df.iloc[idx + sequence_size][target_column] >= df.iloc[idx + sequence_size - 1][target_column] else 0\n",
    "    )\n",
    "    y_validation_date.append(date_list[idx + sequence_size])\n",
    "    row_cursor += 1\n",
    "\n",
    "  X_validation = torch.stack(X_validation_list, dim=0).to(torch.float)\n",
    "  y_validation_regression = torch.tensor(y_validation_regression_list, dtype=torch.float32) / y_normalizer\n",
    "  y_validation_classification = torch.tensor(y_validation_classification_list, dtype=torch.int64)\n",
    "\n",
    "  X_validation = (X_validation - m) / s\n",
    "  #################################################################################################\n",
    "\n",
    "  X_test_list = []\n",
    "  y_test_regression_list = []\n",
    "  y_test_classification_list = []\n",
    "  y_test_date = []\n",
    "  for idx in range(row_cursor, row_cursor + test_size):\n",
    "    sequence_data = df.iloc[idx: idx + sequence_size].values  # sequence_data.shape: (sequence_size, 5)\n",
    "    X_test_list.append(torch.from_numpy(sequence_data))\n",
    "    y_test_regression_list.append(df.iloc[idx + sequence_size][target_column])\n",
    "    y_test_classification_list.append(\n",
    "      1 if df.iloc[idx + sequence_size][target_column] > df.iloc[idx + sequence_size - 1][target_column] else 0\n",
    "    )\n",
    "    y_test_date.append(date_list[idx + sequence_size])\n",
    "    row_cursor += 1\n",
    "\n",
    "  X_test = torch.stack(X_test_list, dim=0).to(torch.float)\n",
    "  y_test_regression = torch.tensor(y_test_regression_list, dtype=torch.float32) / y_normalizer\n",
    "  y_test_classification = torch.tensor(y_test_classification_list, dtype=torch.int64)\n",
    "\n",
    "  X_test = (X_test - m) / s\n",
    "\n",
    "  if is_regression:\n",
    "    return (\n",
    "      X_train, X_validation, X_test,\n",
    "      y_train_regression, y_validation_regression, y_test_regression,\n",
    "      y_train_date, y_validation_date, y_test_date\n",
    "    )\n",
    "  else:\n",
    "    return (\n",
    "      X_train, X_validation, X_test,\n",
    "      y_train_classification, y_validation_classification, y_test_classification,\n",
    "      y_train_date, y_validation_date, y_test_date\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  is_regression = False\n",
    "\n",
    "  X_train, X_validation, X_test, y_train, y_validation, y_test, y_train_date, y_validation_date, y_test_date \\\n",
    "    = get_cryptocurrency_data(\n",
    "    sequence_size=10, validation_size=100, test_size=10,\n",
    "    target_column='Close', y_normalizer=1.0e7, is_regression=is_regression\n",
    "  )\n",
    "\n",
    "  train_crypto_currency_dataset = CryptoCurrencyDataset(X=X_train, y=y_train, is_regression=is_regression)\n",
    "  validation_crypto_currency_dataset = CryptoCurrencyDataset(X=X_validation, y=y_validation, is_regression=is_regression)\n",
    "  test_crypto_currency_dataset = CryptoCurrencyDataset(X=X_test, y=y_test, is_regression=is_regression)\n",
    "\n",
    "  train_data_loader = DataLoader(\n",
    "    dataset=train_crypto_currency_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    "  )\n",
    "\n",
    "  for idx, batch in enumerate(train_data_loader):\n",
    "    input, target = batch\n",
    "    print(\"{0} - {1}: {2}, {3}\".format(idx, input.shape, target.shape, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gg"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
